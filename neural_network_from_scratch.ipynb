{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Réseau de Neurones à partir de zéro pour la Classification du dataset MNIST**"
      ],
      "metadata": {
        "id": "Nq8Gg7f3hXIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importation des bibliothèques"
      ],
      "metadata": {
        "id": "msNobnyJycXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "7kbDLHMFyfzr"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement et prétraitement des données"
      ],
      "metadata": {
        "id": "F37HNgHeyl06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chargement des données MNIST\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X = mnist.data / 255.0  # Normalisation des images\n",
        "y = mnist.target.astype(int)\n",
        "\n",
        "# Séparation des données d'entraînement et de test\n",
        "X_train, X_test = X[:60000], X[60000:]\n",
        "y_train, y_test = y[:60000], y[60000:]\n"
      ],
      "metadata": {
        "id": "mg9HlmyqygyL"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Création du modèle de réseau de neurones et optimisation avec Descente de Gradient"
      ],
      "metadata": {
        "id": "sXuGXb-rytj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Création du modèle de réseau de neurones (2 couches)\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialisation des poids\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Propagation avant\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = np.tanh(self.z1)  # Activation tanh\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.softmax(self.z2)  # Activation softmax pour la sortie\n",
        "        return self.a2\n",
        "\n",
        "    def softmax(self, z):\n",
        "        e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return e_z / np.sum(e_z, axis=1, keepdims=True)\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        m = y_true.shape[0]\n",
        "        log_likelihood = -np.log(y_pred[range(m), y_true])\n",
        "        loss = np.sum(log_likelihood) / m\n",
        "        return loss\n",
        "\n",
        "    def backward(self, X, y_true, learning_rate=0.01):\n",
        "        m = X.shape[0]\n",
        "\n",
        "        # Calcul des gradients\n",
        "        delta2 = self.a2\n",
        "        delta2[range(m), y_true] -= 1\n",
        "        delta2 /= m\n",
        "\n",
        "        dW2 = np.dot(self.a1.T, delta2)\n",
        "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
        "\n",
        "        delta1 = np.dot(delta2, self.W2.T) * (1 - np.square(self.a1))\n",
        "        dW1 = np.dot(X.T, delta1)\n",
        "        db1 = np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "        # Mise à jour des poids et biais\n",
        "        self.W1 -= learning_rate * dW1\n",
        "        self.b1 -= learning_rate * db1\n",
        "        self.W2 -= learning_rate * dW2\n",
        "        self.b2 -= learning_rate * db2\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = self.forward(X)\n",
        "        return np.argmax(output, axis=1)\n"
      ],
      "metadata": {
        "id": "h2CIWMWMyxsP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Fonction d'entraînement du modèle"
      ],
      "metadata": {
        "id": "nTwzRI_6y3OZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraînement du modèle\n",
        "def train(model, X_train, y_train, epochs=10, batch_size=64, learning_rate=0.1):\n",
        "    for epoch in range(epochs):\n",
        "        num_batches = len(X_train) // batch_size\n",
        "        for i in range(num_batches):\n",
        "            X_batch = X_train[i * batch_size:(i + 1) * batch_size]\n",
        "            y_batch = y_train[i * batch_size:(i + 1) * batch_size]\n",
        "\n",
        "            # Propagation avant et arrière\n",
        "            y_pred = model.forward(X_batch)\n",
        "            model.backward(X_batch, y_batch, learning_rate)\n",
        "\n",
        "        # Calcul de la perte pour chaque époque\n",
        "        y_pred_train = model.forward(X_train)\n",
        "        loss = model.compute_loss(y_train, y_pred_train)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss}\")\n"
      ],
      "metadata": {
        "id": "2nxaLcHzy09L"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fonction de test du modèle"
      ],
      "metadata": {
        "id": "kxvaW8p0zBqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test du modèle\n",
        "def test(model, X_test, y_test):\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    accuracy = np.mean(y_pred_test == y_test)\n",
        "    print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "icn8fTojzDcp"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialisation et entraînement du modèle"
      ],
      "metadata": {
        "id": "h362iUvkzH_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation du modèle\n",
        "input_size = 784  # 28x28 pixels\n",
        "hidden_size = 64\n",
        "output_size = 10  # 10 classes (0-9)\n",
        "\n",
        "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "\n",
        "# Entraînement du modèle\n",
        "train(model, X_train.values, y_train.values, epochs=10, batch_size=64, learning_rate=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsHTVTQEzKFB",
        "outputId": "165e95f6-5d50-44ed-f3fd-7488cddff604"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.29214395026136525\n",
            "Epoch 2/10, Loss: 0.22081237990574976\n",
            "Epoch 3/10, Loss: 0.1795830711192649\n",
            "Epoch 4/10, Loss: 0.15263296139325194\n",
            "Epoch 5/10, Loss: 0.13331351528047491\n",
            "Epoch 6/10, Loss: 0.11873315967855869\n",
            "Epoch 7/10, Loss: 0.10728217898513484\n",
            "Epoch 8/10, Loss: 0.09795688578072521\n",
            "Epoch 9/10, Loss: 0.09013167813470323\n",
            "Epoch 10/10, Loss: 0.08340556087178765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test du modèle"
      ],
      "metadata": {
        "id": "XtLfhjgjzQ1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test du modèle\n",
        "test(model, X_test.values, y_test.values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UKueFFfzTcK",
        "outputId": "f3692437-816e-4e29-f24a-acff8979b573"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 96.76%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Création du modèle et optimisation avec momentum"
      ],
      "metadata": {
        "id": "ySTTviv4zoJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Création du modèle de réseau de neurones (2 couches) avec momentum\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, momentum=0.9):\n",
        "        # Initialisation des poids\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "        # Initialisation des vitesses pour momentum\n",
        "        self.vW1 = np.zeros_like(self.W1)\n",
        "        self.vb1 = np.zeros_like(self.b1)\n",
        "        self.vW2 = np.zeros_like(self.W2)\n",
        "        self.vb2 = np.zeros_like(self.b2)\n",
        "\n",
        "        # Paramètre de momentum\n",
        "        self.momentum = momentum\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Propagation avant\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = np.tanh(self.z1)  # Activation tanh\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.softmax(self.z2)  # Activation softmax pour la sortie\n",
        "        return self.a2\n",
        "\n",
        "    def softmax(self, z):\n",
        "        e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return e_z / np.sum(e_z, axis=1, keepdims=True)\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        m = y_true.shape[0]\n",
        "        log_likelihood = -np.log(y_pred[range(m), y_true])\n",
        "        loss = np.sum(log_likelihood) / m\n",
        "        return loss\n",
        "\n",
        "    def backward(self, X, y_true, learning_rate=0.01):\n",
        "        m = X.shape[0]\n",
        "\n",
        "        # Calcul des gradients\n",
        "        delta2 = self.a2\n",
        "        delta2[range(m), y_true] -= 1\n",
        "        delta2 /= m\n",
        "\n",
        "        dW2 = np.dot(self.a1.T, delta2)\n",
        "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
        "\n",
        "        delta1 = np.dot(delta2, self.W2.T) * (1 - np.square(self.a1))\n",
        "        dW1 = np.dot(X.T, delta1)\n",
        "        db1 = np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "        # Mise à jour avec momentum\n",
        "        self.vW1 = self.momentum * self.vW1 - learning_rate * dW1\n",
        "        self.vb1 = self.momentum * self.vb1 - learning_rate * db1\n",
        "        self.vW2 = self.momentum * self.vW2 - learning_rate * dW2\n",
        "        self.vb2 = self.momentum * self.vb2 - learning_rate * db2\n",
        "\n",
        "        # Mise à jour des poids et biais avec la vitesse\n",
        "        self.W1 += self.vW1\n",
        "        self.b1 += self.vb1\n",
        "        self.W2 += self.vW2\n",
        "        self.b2 += self.vb2\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = self.forward(X)\n",
        "        return np.argmax(output, axis=1)\n"
      ],
      "metadata": {
        "id": "YobWRPSazqsH"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fonction d'entraînement du modèle avec momentum"
      ],
      "metadata": {
        "id": "P7dh6T5Vd_jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entraînement du modèle avec momentum\n",
        "def train(model, X_train, y_train, epochs=10, batch_size=64, learning_rate=0.1):\n",
        "    for epoch in range(epochs):\n",
        "        num_batches = len(X_train) // batch_size\n",
        "        for i in range(num_batches):\n",
        "            X_batch = X_train[i * batch_size:(i + 1) * batch_size]\n",
        "            y_batch = y_train[i * batch_size:(i + 1) * batch_size]\n",
        "\n",
        "            # Propagation avant et arrière\n",
        "            y_pred = model.forward(X_batch)\n",
        "            model.backward(X_batch, y_batch, learning_rate)\n",
        "\n",
        "        # Calcul de la perte pour chaque époque\n",
        "        y_pred_train = model.forward(X_train)\n",
        "        loss = model.compute_loss(y_train, y_pred_train)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss}\")\n"
      ],
      "metadata": {
        "id": "jqsOBNnceD87"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialisation et entraînement du modèle avec momentum"
      ],
      "metadata": {
        "id": "o9jRRHSJ0BzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation du modèle avec momentum\n",
        "input_size = 784  # 28x28 pixels\n",
        "hidden_size = 64\n",
        "output_size = 10  # 10 classes (0-9)\n",
        "\n",
        "model = NeuralNetwork(input_size, hidden_size, output_size, momentum=0.9)\n",
        "\n",
        "# Entraînement du modèle\n",
        "train(model, X_train.values, y_train.values, epochs=10, batch_size=64, learning_rate=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z72PeIU0Df3",
        "outputId": "7b69e041-e1e7-4563-c30c-9ec1d063017f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.15206974050626443\n",
            "Epoch 2/10, Loss: 0.11782897032571822\n",
            "Epoch 3/10, Loss: 0.09512212319330719\n",
            "Epoch 4/10, Loss: 0.08180251355546858\n",
            "Epoch 5/10, Loss: 0.08717723657438031\n",
            "Epoch 6/10, Loss: 0.0780436682293898\n",
            "Epoch 7/10, Loss: 0.06621340438758468\n",
            "Epoch 8/10, Loss: 0.05662482909482049\n",
            "Epoch 9/10, Loss: 0.05972824059203405\n",
            "Epoch 10/10, Loss: 0.05230104274669498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test du modèle"
      ],
      "metadata": {
        "id": "UGuXNEaJ0JAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test du modèle\n",
        "test(model, X_test.values, y_test.values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gos5rwxj0M9E",
        "outputId": "685ac19a-f004-4b02-969c-408273910a8d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 96.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Création du modèle et optimisation avec Adagrad"
      ],
      "metadata": {
        "id": "biGZ5YVEetA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Création du modèle de réseau de neurones (2 couches) avec Adagrad\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, epsilon=1e-8):\n",
        "        # Initialisation des poids\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "        # Initialisation des accumulateurs de gradients (pour Adagrad)\n",
        "        self.GW1 = np.zeros_like(self.W1)\n",
        "        self.Gb1 = np.zeros_like(self.b1)\n",
        "        self.GW2 = np.zeros_like(self.W2)\n",
        "        self.Gb2 = np.zeros_like(self.b2)\n",
        "\n",
        "        # Paramètre epsilon pour éviter la division par zéro\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Propagation avant\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = np.tanh(self.z1)  # Activation tanh\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.softmax(self.z2)  # Activation softmax pour la sortie\n",
        "        return self.a2\n",
        "\n",
        "    def softmax(self, z):\n",
        "        e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return e_z / np.sum(e_z, axis=1, keepdims=True)\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        m = y_true.shape[0]\n",
        "        log_likelihood = -np.log(y_pred[range(m), y_true])\n",
        "        loss = np.sum(log_likelihood) / m\n",
        "        return loss\n",
        "\n",
        "    def backward(self, X, y_true, learning_rate=0.01):\n",
        "        m = X.shape[0]\n",
        "\n",
        "        # Calcul des gradients\n",
        "        delta2 = self.a2\n",
        "        delta2[range(m), y_true] -= 1\n",
        "        delta2 /= m\n",
        "\n",
        "        dW2 = np.dot(self.a1.T, delta2)\n",
        "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
        "\n",
        "        delta1 = np.dot(delta2, self.W2.T) * (1 - np.square(self.a1))\n",
        "        dW1 = np.dot(X.T, delta1)\n",
        "        db1 = np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "        # Mise à jour des accumulateurs pour Adagrad\n",
        "        self.GW1 += dW1**2\n",
        "        self.Gb1 += db1**2\n",
        "        self.GW2 += dW2**2\n",
        "        self.Gb2 += db2**2\n",
        "\n",
        "        # Mise à jour des poids et biais avec Adagrad\n",
        "        self.W1 -= learning_rate * dW1 / (np.sqrt(self.GW1) + self.epsilon)\n",
        "        self.b1 -= learning_rate * db1 / (np.sqrt(self.Gb1) + self.epsilon)\n",
        "        self.W2 -= learning_rate * dW2 / (np.sqrt(self.GW2) + self.epsilon)\n",
        "        self.b2 -= learning_rate * db2 / (np.sqrt(self.Gb2) + self.epsilon)\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = self.forward(X)\n",
        "        return np.argmax(output, axis=1)\n"
      ],
      "metadata": {
        "id": "FEBObXld0oEg"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialisation et entraînement du modèle avec Adagrad"
      ],
      "metadata": {
        "id": "ujaxjPAgg9da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation du modèle avec Adagrad\n",
        "input_size = 784  # 28x28 pixels\n",
        "hidden_size = 64\n",
        "output_size = 10  # 10 classes (0-9)\n",
        "\n",
        "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "\n",
        "# Entraînement du modèle\n",
        "train(model, X_train.values, y_train.values, epochs=10, batch_size=64, learning_rate=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPgB1Qw-0xvw",
        "outputId": "d6274d45-ab0f-4612-8377-104544460725"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.8900416888062903\n",
            "Epoch 2/10, Loss: 0.876431567826437\n",
            "Epoch 3/10, Loss: 0.8715988735979321\n",
            "Epoch 4/10, Loss: 0.8697767930073292\n",
            "Epoch 5/10, Loss: 0.8689500252623208\n",
            "Epoch 6/10, Loss: 0.8684823789709594\n",
            "Epoch 7/10, Loss: 0.868148706843655\n",
            "Epoch 8/10, Loss: 0.8679259948085648\n",
            "Epoch 9/10, Loss: 0.8677811178014015\n",
            "Epoch 10/10, Loss: 0.8676088639955631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test du modèle"
      ],
      "metadata": {
        "id": "T02D9FehgyBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test du modèle\n",
        "test(model, X_test.values, y_test.values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPhfswTa07Hy",
        "outputId": "28b566cd-4a23-4502-b908-a90080a3e92c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 96.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implémentation du réseau de neurones avec mise à jour des poids via Proximal Stochastic Approximation (PSA)"
      ],
      "metadata": {
        "id": "h0vWGDrQf9JU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Création du modèle de réseau de neurones (2 couches)\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, lambda_psa=0.1):\n",
        "        # Initialisation des poids\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "        self.lambda_psa = lambda_psa  # Paramètre de régularisation PSA\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Propagation avant\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = np.tanh(self.z1)  # Activation tanh\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.softmax(self.z2)  # Activation softmax pour la sortie\n",
        "        return self.a2\n",
        "\n",
        "    def softmax(self, z):\n",
        "        e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return e_z / np.sum(e_z, axis=1, keepdims=True)\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        m = y_true.shape[0]\n",
        "        log_likelihood = -np.log(y_pred[range(m), y_true])\n",
        "        loss = np.sum(log_likelihood) / m\n",
        "        return loss\n",
        "\n",
        "    def backward(self, X, y_true, learning_rate=0.01):\n",
        "        m = X.shape[0]\n",
        "\n",
        "        # Calcul des gradients\n",
        "        delta2 = self.a2\n",
        "        delta2[range(m), y_true] -= 1\n",
        "        delta2 /= m\n",
        "\n",
        "        dW2 = np.dot(self.a1.T, delta2)\n",
        "        db2 = np.sum(delta2, axis=0, keepdims=True)\n",
        "\n",
        "        delta1 = np.dot(delta2, self.W2.T) * (1 - np.square(self.a1))\n",
        "        dW1 = np.dot(X.T, delta1)\n",
        "        db1 = np.sum(delta1, axis=0, keepdims=True)\n",
        "\n",
        "        # PSA update: appliquer un terme de régularisation sur les poids (Proximal)\n",
        "        self.W1 -= learning_rate * (dW1 + self.lambda_psa * self.W1)  # Proximal pour W1\n",
        "        self.b1 -= learning_rate * db1\n",
        "        self.W2 -= learning_rate * (dW2 + self.lambda_psa * self.W2)  # Proximal pour W2\n",
        "        self.b2 -= learning_rate * db2\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = self.forward(X)\n",
        "        return np.argmax(output, axis=1)\n"
      ],
      "metadata": {
        "id": "2ORclavB1eij"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialisation et entraînement du modèle avec PSA"
      ],
      "metadata": {
        "id": "oy_uQe_NhDJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation du modèle\n",
        "input_size = 784  # 28x28 pixels\n",
        "hidden_size = 64\n",
        "output_size = 10  # 10 classes (0-9)\n",
        "lambda_psa = 0.1  # Paramètre de régularisation PSA\n",
        "\n",
        "model = NeuralNetwork(input_size, hidden_size, output_size, lambda_psa)\n",
        "\n",
        "# Entraînement du modèle\n",
        "train(model, X_train.values, y_train.values, epochs=10, batch_size=64, learning_rate=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep-3hwsjX7_i",
        "outputId": "05462c03-a86b-4bf3-bf15-6e8bc6d6e96c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.8861966979405707\n",
            "Epoch 2/10, Loss: 0.874387282161788\n",
            "Epoch 3/10, Loss: 0.8713493574683842\n",
            "Epoch 4/10, Loss: 0.8703042617739509\n",
            "Epoch 5/10, Loss: 0.8695128386812778\n",
            "Epoch 6/10, Loss: 0.8687448810588737\n",
            "Epoch 7/10, Loss: 0.8680075390808283\n",
            "Epoch 8/10, Loss: 0.8675548168972784\n",
            "Epoch 9/10, Loss: 0.8672945959214289\n",
            "Epoch 10/10, Loss: 0.8671524854892657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test du modèle"
      ],
      "metadata": {
        "id": "8-q6sAVrg2kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test du modèle\n",
        "test(model, X_test.values, y_test.values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0ady-MgYM3Z",
        "outputId": "56736771-95b1-4b76-f0ae-4f59f98dacd5"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 84.21%\n"
          ]
        }
      ]
    }
  ]
}